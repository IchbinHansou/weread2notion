{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IchbinHansou/weread2notion/blob/main/LDS_ESSLLI_2024_Monday_Worksheet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lingusitic Data Science - Monday\n",
        "\n",
        "In this worksheet we will look at doing author identification using simple statistics\n",
        "\n",
        "Our first step is to get some data. We will use texts from Project Gutenburg that we will access using NLTK"
      ],
      "metadata": {
        "id": "QDfNPyzlEUSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"gutenberg\")\n",
        "nltk.download(\"punkt\") # to enable tokenization, we will need this later\n",
        "from nltk.corpus import gutenberg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7DSis59Ejo1",
        "outputId": "f0b13221-3232-40c3-fabb-06d46aa37468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can examine the files in this corpus by using the `fileids()` function.\n",
        "\n",
        "In Python we can use square brackets to access `[]` an element in an array. Note that array elements start from `0`.\n",
        "\n",
        "__Question:__ What is the 5th document in the corpus?"
      ],
      "metadata": {
        "id": "x398UxfpE4Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = gutenberg.fileids()[0]\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wazYEHUDFDAv",
        "outputId": "4dc34d41-eeb5-4fb2-e14b-09e90a2bfd4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "austen-emma.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can access the content of the documents in several ways:\n",
        "\n",
        "* `gutenberg.raw(doc)`\n",
        "* `gutenberg.words(doc)`\n",
        "* `gutenberg.sents(doc)`\n",
        "\n",
        "__Question:__ Try each of these and compare the results. Try accessing the 5th element of each to compare the results."
      ],
      "metadata": {
        "id": "fq6VT4TWFkqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gutenberg.sents(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZysGIDpxGDcs",
        "outputId": "5a74822e-5797-46a3-c00f-e7f25f7e2843"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['[', 'Emma', 'by', 'Jane', 'Austen', '1816', ']'], ['VOLUME', 'I'], ...]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can build up a count of the words using a built-in class called `Counter` from Python"
      ],
      "metadata": {
        "id": "U-hTDCbWGqbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the class into the environment\n",
        "from collections import Counter\n",
        "\n",
        "# Create a new word frequency counter\n",
        "word_frequencies = Counter()\n",
        "\n",
        "# Iterate through each word in the document\n",
        "for word in gutenberg.words(doc):\n",
        "  # Note that in Python indentation indicates we are inside the loop\n",
        "  # We increment the count by 1\n",
        "  word_frequencies[word] = word_frequencies[word] + 1"
      ],
      "metadata": {
        "id": "HzDrl5SRG1Ol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Question:__ Can you change the code above to calculate the word frequency over all words in all documents.\n",
        "\n",
        "The function `lower` can be used to convert a string to lowercase\n",
        "\n",
        "```python\n",
        "\"ExAmPle\".lower() == \"example\"\n",
        "```\n",
        "\n",
        "__Question:__ Can you further modify your code to be case-invariant?"
      ],
      "metadata": {
        "id": "Pq_Gx4HvHcVZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to compare two frequency distributions we will use _cosine similarity_ that can be defined as follows\n",
        "\n",
        "$\n",
        "cos(x,y) = \\frac{\\sum_i x_i \\times y_i}{\\sqrt{\\sum_i x_i^2 \\sum_i y_i^2}}\n",
        "$\n",
        "\n",
        "I give an implementation of this below (don't worry if you don't quite understand this!)"
      ],
      "metadata": {
        "id": "OvpTMx_mIe27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "\n",
        "def cosine(x, y):\n",
        "  xy = 0\n",
        "  xx = 0\n",
        "  yy = 0\n",
        "  for w1, f1 in x.items():\n",
        "    xy += f1 * y[w1]\n",
        "    xx += f1 * f1\n",
        "  for _, f2 in y.items():\n",
        "    yy += f2 * f2\n",
        "  if xx == 0 or yy == 0:\n",
        "    return 0\n",
        "  return xy / sqrt(xx * yy)"
      ],
      "metadata": {
        "id": "TwoOXkCsI6zV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now divide our distribution into two. One for works written by Shakespeare and one for works not written by Shakespeare"
      ],
      "metadata": {
        "id": "wJbW4qsdJTUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shakespeare = Counter()\n",
        "not_shakespeare = Counter()\n",
        "\n",
        "for file_id in gutenberg.fileids():\n",
        "  for word in gutenberg.words(file_id):\n",
        "    if file_id.startswith(\"shakespeare\"):\n",
        "      shakespeare[word] += 1\n",
        "    else:\n",
        "      not_shakespeare[word] += 1"
      ],
      "metadata": {
        "id": "HKOqaDyXJojo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Question:__ Using the code above compute the cosine similarity between `shakespeare` and `not_shakespeare` for the documents `shakespeare-caesar.txt` and `milton-paradise.txt`.\n",
        "\n",
        "What does this tell you?"
      ],
      "metadata": {
        "id": "GStpTtJVKIuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "caesar = Counter()\n",
        "paradise = Counter()\n",
        "\n",
        "\n",
        "for file_id in [\"shakespeare-caesar.txt\",\"milton-paradise.txt\"]:\n",
        "  for word in gutenberg.words(file_id):\n",
        "    if file_id.startswith(\"shakespeare\"):\n",
        "      caesar[word] += 1\n",
        "    else:\n",
        "      paradise[word] += 1\n",
        "\n",
        "print(\"Is Caesar            by Shakespeare:\", cosine(caesar, shakespeare))\n",
        "print(\"Is Caesar        not by Shakespeare:\", cosine(caesar, not_shakespeare))\n",
        "print(\"Is Paradise Lost     by Shakespeare:\", cosine(paradise, shakespeare))\n",
        "print(\"Is Paradise Lost not by Shakespeare:\",cosine(paradise, not_shakespeare))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQS3S3WwK3w_",
        "outputId": "b1fe5e5a-899a-4e61-dfa6-4ec71c2bec8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Is Caesar            by Shakespeare: 0.9902101010373634\n",
            "Is Caesar        not by Shakespeare: 0.8785200083947318\n",
            "Is Paradise Lost     by Shakespeare: 0.8646398762071504\n",
            "Is Paradise Lost not by Shakespeare: 0.894796702901323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Question:__ Finally, we notice that we are using the two texts to calculate the frequencies for `caesar` and `paradise`. Modify the code above to remove these documents? Does the result still work?"
      ],
      "metadata": {
        "id": "696hl1RWM9ab"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Buzz groups\n",
        "Discuss any further improvements to this methodology that you can think of."
      ],
      "metadata": {
        "id": "PnYAENuFNPBV"
      }
    }
  ]
}